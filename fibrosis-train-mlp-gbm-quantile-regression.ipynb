{"cells":[{"metadata":{},"cell_type":"markdown","source":"Lung Fibrosis Training notebook for MLP, Quantile Regression and Graident Boost Tree algorithm"},{"metadata":{},"cell_type":"markdown","source":"# MLP"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"    #import library\n    import numpy as np\n    import pandas as pd\n    import pydicom\n    import os\n    import random\n    import matplotlib.pyplot as plt\n    from tqdm import tqdm\n    from PIL import Image\n    from sklearn.metrics import mean_absolute_error\n    from sklearn.model_selection import KFold\n\n    import tensorflow as tf\n    import tensorflow.keras.backend as K\n    import tensorflow.keras.layers as L\n    import tensorflow.keras.models as M\n    #import autokeras as ak","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#set seeding\ndef seed_everything(seed=2020):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    \nseed_everything(22)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#set root path\nROOT = \"../input/osic-pulmonary-fibrosis-progression\"\n#set batch size\nBATCH_SIZE=128","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#read in train and test\ntr = pd.read_csv(f\"{ROOT}/train.csv\")\ntr.drop_duplicates(keep=False, inplace=True, subset=['Patient','Weeks'])\nchunk = pd.read_csv(f\"{ROOT}/test.csv\")\n\n# read in sample submission file\nprint(\"add infos\")\nsub = pd.read_csv(f\"{ROOT}/sample_submission.csv\")\nsub['Patient'] = sub['Patient_Week'].apply(lambda x:x.split('_')[0])\nsub['Weeks'] = sub['Patient_Week'].apply(lambda x: int(x.split('_')[-1]))\nsub =  sub[['Patient','Weeks','Confidence','Patient_Week']]\nsub = sub.merge(chunk.drop('Weeks', axis=1), on=\"Patient\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"tr.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"chunk.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"sub.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#combine into one dataframe for preprocessing\ntr['WHERE'] = 'train'\nchunk['WHERE'] = 'val'\nsub['WHERE'] = 'test'\ndata = tr.append([chunk])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#print out shape for train, validation and test and number of unique record\nprint(tr.shape, chunk.shape, data.shape)\nprint(tr.Patient.nunique(), chunk.Patient.nunique(), \n      data.Patient.nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#calcuate the minimal week for each patient, given there are multiple weeks of scan for each patient, feature engineering\ndata['min_week'] = data['Weeks']\ndata.loc[data.WHERE=='test','min_week'] = np.nan\ndata['min_week'] = data.groupby('Patient')['min_week'].transform('min')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#filter out patient that have only 1 week of FCV\n\nbase = data.loc[data.Weeks == data.min_week]\nbase = base[['Patient','FVC']].copy()\nbase.columns = ['Patient','min_FVC']\nbase['nb'] = 1\nbase['nb'] = base.groupby('Patient')['nb'].transform('cumsum')\nbase = base[base.nb==1]\nbase.drop('nb', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"base.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create data frame that show min and current week & FCV for each patient, dups exist\ndata = data.merge(base, on='Patient', how='left')\ndata['base_week'] = data['Weeks'] - data['min_week']\ndel base","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating dummy variable for categorical variables\n\nCOLS = ['Sex','SmokingStatus'] #,'Age'\nFE = []\nfor col in COLS:\n    for mod in data[col].unique():\n        FE.append(mod)\n        data[mod] = (data[col] == mod).astype(int)\n#=================","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalize the numerical values using min-max \n\ndata['age'] = (data['Age'] - data['Age'].min() ) / ( data['Age'].max() - data['Age'].min() )\ndata['BASE'] = (data['min_FVC'] - data['min_FVC'].min() ) / ( data['min_FVC'].max() - data['min_FVC'].min() )\ndata['week'] = (data['base_week'] - data['base_week'].min() ) / ( data['base_week'].max() - data['base_week'].min() )\ndata['percent'] = (data['Percent'] - data['Percent'].min() ) / ( data['Percent'].max() - data['Percent'].min() )\nFE += ['age','percent','week','BASE']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"FE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data.FVC\nx = data.drop(['Patient','Weeks','Percent','Age','Sex','SmokingStatus','WHERE','min_week','base_week','FVC'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"x.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#break aprat to train, validation and test\n#tr = data.loc[data.WHERE=='train']\n#chunk = data.loc[data.WHERE=='val']\n#sub = data.loc[data.WHERE=='test']\n#del data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nxTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xTrain.shape, xTest.shape,yTrain.shape, yTest.shape ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tr.shape, chunk.shape, sub.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yTrain.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### BASELINE NN "},{"metadata":{"trusted":true},"cell_type":"code","source":"C1, C2 = tf.constant(70, dtype='float32'), tf.constant(1000, dtype=\"float32\")\n#=============================#\ndef score(y_true, y_pred):\n    tf.dtypes.cast(y_true, tf.float32)\n    tf.dtypes.cast(y_pred, tf.float32)\n    sigma = y_pred[:, 2] - y_pred[:, 0]\n    fvc_pred = y_pred[:, 1]\n    \n    #sigma_clip = sigma + C1\n    sigma_clip = tf.maximum(sigma, C1)\n    delta = tf.abs(y_true[:, 0] - fvc_pred)\n    delta = tf.minimum(delta, C2)\n    sq2 = tf.sqrt( tf.dtypes.cast(2, dtype=tf.float32) )\n    metric = (delta / sigma_clip)*sq2 + tf.math.log(sigma_clip* sq2)\n    return K.mean(metric)\n#============================#\ndef qloss(y_true, y_pred):\n    # Pinball loss for multiple quantiles\n    qs = [0.2, 0.50, 0.8]\n    q = tf.constant(np.array([qs]), dtype=tf.float32)\n    e = y_true - y_pred\n    v = tf.maximum(q*e, (q-1)*e)\n    return K.mean(v)\n#=============================#\ndef mloss(_lambda):\n    def loss(y_true, y_pred):\n        return _lambda * qloss(y_true, y_pred) + (1 - _lambda)*score(y_true, y_pred)\n    return loss\n#=================\ndef make_model(nh):\n    z = L.Input((nh,), name=\"Patient\")\n    x = L.Dense(1000, activation=\"relu\", name=\"d1\")(z)\n    x = L.Dense(1000, activation=\"relu\", name=\"d2\")(x)\n    #x = L.BatchNormalization()(x)\n    #x = L.Dense(1000, activation=\"relu\", name=\"d3\")(x)\n    #x = L.BatchNormalization()(x)\n    #x = L.Dropout(.3)(x)\n    \n    #x = L.Dense(1000, activation=\"relu\", name=\"d3\")(x)\n    p1 = L.Dense(3, activation=\"linear\", name=\"p1\")(x)\n    p2 = L.Dense(3, activation=\"relu\", name=\"p2\")(x)\n    preds = L.Lambda(lambda x: x[0] + tf.cumsum(x[1], axis=1), \n                     name=\"preds\")([p1, p2])\n    \n    model = M.Model(z, preds, name=\"CNN\")\n    #model.compile(loss=qloss, optimizer=\"adam\", metrics=[score])\n    model.compile(loss=mloss(0.8), optimizer=tf.keras.optimizers.Adam(lr=0.02, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.01, amsgrad=False), metrics=[score])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#y1 = tr['FVC'].values\n#z1 = tr[FE].values\n#ze = sub[FE].values\n#nh = z.shape[1]\n#pe = np.zeros((ze.shape[0], 3))\n#pred = np.zeros((z.shape[0], 3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = y.values\nz = x.values\n#ze = sub[FE].values\nnh = z.shape[1]\n#pe = np.zeros((ze.shape[0], 3))\npred = np.zeros((z.shape[0], 3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"y = yTrain.values\nz = xTrain.values\n#ze = sub[FE].values\nnh = z.shape[1]\n#pe = np.zeros((ze.shape[0], 3))\npred = np.zeros((z.shape[0], 3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"net = make_model(nh)\nprint(net.summary())\nprint(net.count_params())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NFOLD = 5\nkf = KFold(n_splits=NFOLD)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"print(z.shape)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"history = net.fit(z, y, validation_split=0.2, epochs=10, batch_size=128, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"print(history.history.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"print(history.history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['score'])\nplt.plot(history.history['val_score'])\nplt.title('model score')\nplt.ylabel('score')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"%%time\ncnt = 0\nEPOCHS = 1000\nfor tr_idx, val_idx in kf.split(z):\n    cnt += 1\n    print(f\"FOLD {cnt}\")\n    net = make_model(nh)\n    net.fit(z[tr_idx], y[tr_idx], batch_size=BATCH_SIZE, epochs=EPOCHS, \n            validation_data=(z[val_idx], y[val_idx]), verbose=0) #\n    print(\"train\", net.evaluate(z[tr_idx], y[tr_idx], verbose=0, batch_size=BATCH_SIZE))\n    print(\"val\", net.evaluate(z[val_idx], y[val_idx], verbose=0, batch_size=BATCH_SIZE))\n    print(\"predict val...\")\n    pred[val_idx] = net.predict(z[val_idx], batch_size=BATCH_SIZE, verbose=0)\n    print(\"predict test...\")\n    #pe += net.predict(ze, batch_size=BATCH_SIZE, verbose=0) / NFOLD\n#==============","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predict validation test\npredy = net.predict(z,verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"predy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sigma_opt = mean_absolute_error(y, pred[:, 1])\n#unc = pred[:,2] - pred[:, 0]\n#sigma_mean = np.mean(unc)\nprint(sigma_opt, sigma_mean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idxs = np.random.randint(0, y.shape[0], 100)\nplt.plot(y[idxs], label=\"ground truth\")\nplt.plot(pred[idxs, 0], label=\"q25\")\n#plt.plot(pred[idxs, 1], label=\"q50\")\n#plt.plot(pred[idxs, 2], label=\"q75\")#plt.legend(loc=\"best\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(unc.min(), unc.mean(), unc.max(), (unc>=0).mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(unc)\nplt.title(\"uncertainty in prediction\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train['ypredL'] = modelL.predict( train ).values\n#train['ypred']  = model.predict( train ).values\n#train['ypredH'] = modelH.predict( train ).values\n\n\n#train['ypredstd'] = 0.5*np.abs(train['ypredH'] - train['ypred'])+0.5*np.abs(train['ypred'] - train['ypredL'])\n#train.head(10)\n\n\n#predictstd = 0.5*np.abs(pred[val_idx][:,2] - pred[val_idx][:,1])+0.5*np.abs(pred[val_idx][:,1] - pred[val_idx][:,0])\n\npredictstdy = 0.5*np.abs(predy[:,2] - predy[:,1])+0.5*np.abs(predy[:,1] - predy[:,0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def metric( trueFVC, predFVC, predSTD ):\n    \n    clipSTD = np.clip( predSTD, 70 , 9e9 )  \n    \n    deltaFVC = np.clip( np.abs(trueFVC-predFVC), 0 , 1000 )  \n\n    return np.mean( -1*(np.sqrt(2)*deltaFVC/clipSTD) - np.log( np.sqrt(2)*clipSTD ) )\n    \n\nprint( 'Metric:', metric(yTest.values,  predy[:,1], predictstdy))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### PREDICTION"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['FVC1'] = 0.996*pe[:, 1]\nsub['Confidence1'] = pe[:, 2] - pe[:, 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subm = sub[['Patient_Week','FVC','Confidence','FVC1','Confidence1']].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subm.loc[~subm.FVC1.isnull()].head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subm.loc[~subm.FVC1.isnull(),'FVC'] = subm.loc[~subm.FVC1.isnull(),'FVC1']\nif sigma_mean<70:\n    subm['Confidence'] = sigma_opt\nelse:\n    subm.loc[~subm.FVC1.isnull(),'Confidence'] = subm.loc[~subm.FVC1.isnull(),'Confidence1']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subm.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subm.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"otest = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/test.csv')\nfor i in range(len(otest)):\n    subm.loc[subm['Patient_Week']==otest.Patient[i]+'_'+str(otest.Weeks[i]), 'FVC'] = otest.FVC[i]\n    subm.loc[subm['Patient_Week']==otest.Patient[i]+'_'+str(otest.Weeks[i]), 'Confidence'] = 0.1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subm[[\"Patient_Week\",\"FVC\",\"Confidence\"]].to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Quantile regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"#quantile regression\n\nfrom statsmodels.formula.api import quantreg\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv( '../input/osic-pulmonary-fibrosis-progression/train.csv' )\ntest  = pd.read_csv( '../input/osic-pulmonary-fibrosis-progression/test.csv' )\n\ntrain['traintest'] = 0\ntest ['traintest'] = 1\n\nsub   = pd.read_csv( '../input/osic-pulmonary-fibrosis-progression/sample_submission.csv' )\nsub['Weeks']   = sub['Patient_Week'].apply( lambda x: int(x.split('_')[-1]) )\nsub['Patient'] = sub['Patient_Week'].apply( lambda x: x.split('_')[0] ) \n\nprint( train.shape, test.shape, sub.shape )\n\n#sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Patient.nunique(), sub.Patient.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.concat( (train,test) )\ntrain.sort_values( ['Patient','Weeks'], inplace=True )\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#calcuate the minimal week for each patient, given there are multiple weeks of scan for each patient, feature engineering\ntrain['min_week'] = train['Weeks']\n#train.loc[data.WHERE=='test','min_week'] = np.nan\ntrain['min_week'] = train.groupby('Patient')['min_week'].transform('min')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#filter out patient that have only 1 week of FCV\n\nbase = train.loc[train.Weeks == train.min_week]\nbase = base[['Patient','FVC']].copy()\nbase.columns = ['Patient','min_FVC']\nbase['nb'] = 1\nbase['nb'] = base.groupby('Patient')['nb'].transform('cumsum')\nbase = base[base.nb==1]\nbase.drop('nb', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create data frame that show min and current week & FCV for each patient, dups exist\ndata = train.merge(base, on='Patient', how='left')\ndata['base_week'] = data['Weeks'] - data['min_week']\ndel base","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.groupby( ['Sex','SmokingStatus'] )['FVC'].agg( ['mean','std','count'] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#label enconding\n\ndata['Sex']           = pd.factorize( data['Sex'] )[0]\ndata['SmokingStatus'] = pd.factorize( data['SmokingStatus'] )[0]\ndata.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating z score\n\ndata['Percent']       = (data['Percent'] - data['Percent'].mean()) / data['Percent'].std()\ndata['Age']           = (data['Age'] - data['Age'].mean()) / data['Age'].std()\ndata['Sex']           = (data['Sex'] - data['Sex'].mean()) / data['Sex'].std()\ndata['SmokingStatus'] = (data['SmokingStatus'] - data['SmokingStatus'].mean()) / data['SmokingStatus'].std()\ndata.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data.FVC\nx = data.drop(['Patient','traintest','base_week'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nxTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xTrain.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\n\ncorrmat = xTrain.corr() \nf, ax = plt.subplots(figsize =(9, 8)) \nsns.heatmap(corrmat, ax = ax, cmap = 'RdYlBu_r', linewidths = 0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xTrain.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelL = quantreg('FVC ~ Weeks+Age+Sex+SmokingStatus+min_FVC+Percent+min_week+min_FVC', xTrain).fit( q=0.15 )\nmodel  = quantreg('FVC ~ Weeks+Age+Sex+SmokingStatus+min_FVC+Percent+min_week+min_FVC', xTrain).fit( q=0.50 )\nmodelH = quantreg('FVC ~ Weeks+Age+Sex+SmokingStatus+min_FVC+Percent+min_week+min_FVC', xTrain).fit( q=0.85 )\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yTrain.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xTest['ypredL'] = modelL.predict( xTest ).values\nxTest['ypred']  = model.predict( xTest ).values\nxTest['ypredH'] = modelH.predict( xTest ).values\nxTest['ypredstd'] = 0.5*np.abs(xTest['ypredH'] - xTest['ypred'])+0.5*np.abs(xTest['ypred'] - xTest['ypredL'])\nxTest.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#modelL = quantreg('FVC ~ Weeks+Age+Sex+SmokingStatus', train).fit( q=0.15 )\n#model  = quantreg('FVC ~ Weeks+Age+Sex+SmokingStatus', train).fit( q=0.50 )\n#modelH = quantreg('FVC ~ Weeks+Age+Sex+SmokingStatus', train).fit( q=0.85 )\n#print(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train['ypredL'] = modelL.predict( train ).values\n#train['ypred']  = model.predict( train ).values\n#train['ypredH'] = modelH.predict( train ).values\n#train['ypredstd'] = 0.5*np.abs(train['ypredH'] - train['ypred'])+0.5*np.abs(train['ypred'] - train['ypredL'])\n#train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def metric( trueFVC, predFVC, predSTD ):\n    \n    clipSTD = np.clip( predSTD, 70 , 9e9 )  \n    \n    deltaFVC = np.clip( np.abs(trueFVC-predFVC), 0 , 1000 )  \n\n    return np.mean( -1*(np.sqrt(2)*deltaFVC/clipSTD) - np.log( np.sqrt(2)*clipSTD ) )\n    \n\nprint( 'Metric:', metric( xTest['FVC'].values, xTest['ypred'].values, xTest['ypredstd'].values  ) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train['FVC'].values\nypredL = train['ypredL'].values\nypred = train['ypred'].values\nypredH = train['ypredH'].values\n\nidxs = np.random.randint(0, y.shape[0], 100)\nplt.plot(y[idxs], label=\"ground truth\")\nplt.plot(ypredL[idxs], label=\"q25\")\nplt.plot(ypred[idxs], label=\"q50\")\nplt.plot(ypredH[idxs], label=\"q75\")\nplt.legend(loc=\"best\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sigma_opt_qr = mean_absolute_error(y, ypred)\nunc_qr = ypredH - ypredL\nsigma_mean_qr = np.mean(unc_qr)\nprint(sigma_opt_qr, sigma_mean_qr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(unc_qr)\nplt.title(\"uncertainty in prediction\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# GBM Quantile Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"#GBM regression\ndata.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data.FVC\nx = data.drop(['FVC','Patient'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor(n_estimators = 100,\n                           n_jobs = -1,\n                           oob_score = True,\n                           bootstrap = True,\n                           random_state = 42)\nrf.fit(x, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# udfs ----\n\n# function for creating a feature importance dataframe\ndef imp_df(column_names, importances):\n    df = pd.DataFrame({'feature': column_names,\n                       'feature_importance': importances}) \\\n           .sort_values('feature_importance', ascending = False) \\\n           .reset_index(drop = True)\n    return df\n\n# plotting a feature importance dataframe (horizontal barchart)\ndef var_imp_plot(imp_df, title):\n    imp_df.columns = ['feature', 'feature_importance']\n    sns.barplot(x = 'feature_importance', y = 'feature', data = imp_df, orient = 'h', color = 'royalblue') \\\n       .set_title(title, fontsize = 20)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nbase_imp = imp_df(x.columns, rf.feature_importances_)\nbase_imp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\n\nvar_imp_plot(base_imp, 'Default feature importance (scikit-learn)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#x = train[[\"Weeks\",\"Percent\",\"Age\",\"Sex\",\"SmokingStatus\"]]\n#y = train[[\"FVC\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import machine learning libraries\nimport xgboost as xgb\nfrom sklearn.metrics import accuracy_score\n\n\n# import packages for hyperparameters tuning\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics.regression import mean_absolute_error as mae\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\n\ndef objective_function_regression(estimator):\n    mae_array = cross_val_score( estimator, X_train, y_train, cv= 3, n_jobs=-1, scoring = make_scorer(mae) )\n    return np.mean(mae_array)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\nfrom hyperopt import hp, fmin, tpe\nfrom hyperopt.pyll import scope\n\n# hyperopt object for \nscope.define(GradientBoostingRegressor)\n\n# search space\nn_estimators  = hp.randint('n_estimators',1000) \nlearning_rate = hp.loguniform('learning_rate',-3,1)\nmax_depth     = hp.randint('max_depth', 10)\nmax_features =  hp.randint('max_features',x.shape[1]-1)\nmin_samples_leaf = hp.randint('min_samples_leaf', 10)\nmin_samples_spilt = hp.randint('min_samples_leaf', 10)\n                                       \n# model / estimator to be optimized\nest0 = (0.1, scope.GradientBoostingRegressor( n_estimators  = n_estimators + 1,\n                                            learning_rate = learning_rate,\n                                            max_depth = max_depth + 1,\n                                            max_features = max_features + 1,\n                                            min_samples_leaf = min_samples_leaf + 1,\n                                            min_samples_spilt = min_samples_spilt + 1,                                           \n                                            random_state=42) \n        )\n\n# search space\nsearch_space_regression = hp.pchoice('estimator', [est0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best = fmin(\n    fn= objective_function_regression,\n    space= search_space_regression,\n    algo = tpe.suggest, # This is the optimization algorithm hyperopt uses, a tree of parzen estimators\n    max_evals = 1000,\n    verbose = 1  # The number of iterations\n         )\n\nprint(best)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\nquantiles = [0.15, 0.50, 0.85]\n\n\n# Get the model and the predictions in (a) - (b)\ndef GBM(q):\n    \n   # (a) Modeling  500\n   mod = GradientBoostingRegressor(loss='quantile', alpha=q,\n                                n_estimators=1000, max_depth=5,max_features = 3,\n                                learning_rate=0.05599876950226799, min_samples_leaf=6,\n                                min_samples_split=20)\n   mod.fit(X_train, y_train)\n\n   # (b) Predictions\n   pred = pd.Series(mod.predict(X_test).round(2))\n   return pred, mod\n\nGBM_models=[]\nGBM_actual_pred = pd.DataFrame()\n\nfor q in quantiles:\n    pred , model = GBM(q)\n    GBM_models.append(model)\n    GBM_actual_pred = pd.concat([GBM_actual_pred,pred],axis=1)\n    \nGBM_actual_pred.columns=quantiles\n#GBM_actual_pred['actual'] = y\n#GBM_actual_pred['interval'] = GBM_actual_pred[np.max(quantiles)] - GBM_actual_pred[np.min(quantiles)]\n#GBM_actual_pred = GBM_actual_pred.sort_values('interval')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GBM_actual_pred['actual'] = y_test.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GBM_actual_pred['interval'] = GBM_actual_pred[np.max(quantiles)] - GBM_actual_pred[np.min(quantiles)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GBM_actual_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GBM_actual_pred = GBM_actual_pred.rename(columns={0.15: 'ypredL', 0.5: 'ypred',0.85: 'ypredH'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GBM_actual_pred['ypredstd'] = 0.5*np.abs(GBM_actual_pred['ypredH'] - GBM_actual_pred['ypred'])+0.5*np.abs(GBM_actual_pred['ypred'] - GBM_actual_pred['ypredL'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def metric( trueFVC, predFVC, predSTD ):\n    \n    clipSTD = np.clip( predSTD, 70 , 9e9 )  \n    \n    deltaFVC = np.clip( np.abs(trueFVC-predFVC), 0 , 1000 )  \n\n    return np.mean( -1*(np.sqrt(2)*deltaFVC/clipSTD) - np.log( np.sqrt(2)*clipSTD ) )\n    \n\nprint( 'Metric:', metric( GBM_actual_pred['actual'].values, GBM_actual_pred['ypred'].values, GBM_actual_pred['ypredstd'].values  ) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = GBM_actual_pred['actual'].values\nypredL = GBM_actual_pred['ypredL'].values\nypred = GBM_actual_pred['ypred'].values\nypredH = GBM_actual_pred['ypredH'].values\n\nidxs = np.random.randint(0, y.shape[0], 100)\nplt.plot(y[idxs], label=\"ground truth\")\nplt.plot(ypredL[idxs], label=\"q25\")\nplt.plot(ypred[idxs], label=\"q50\")\nplt.plot(ypredH[idxs], label=\"q75\")\nplt.legend(loc=\"best\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sigma_opt_qr = mean_absolute_error(y, ypred)\nunc_qr = ypredH - ypredL\nsigma_mean_qr = np.mean(unc_qr)\nprint(sigma_opt_qr, sigma_mean_qr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(unc_qr)\nplt.title(\"uncertainty in prediction\")\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}